import os
import json
import random
import argparse
import dill
import torch
from datetime import datetime
from smolagents import TransformersModel, CodeAgent, LogLevel
from smolagents.models import ChatMessage
from custom_tools import *
import inseq
from transformers import AutoTokenizer

# ==============================================================================
# == 1. Function Definition
# ==============================================================================

def run_my_agent(task_prompt: str, tools_to_use: list, model: TransformersModel) -> str:
    """
    Runs the LLM agent.
    
    Args:
        task_prompt: The input string (=task prompt) for the agent.
        
    Returns:
        The full log of the agent run as a list object.
    """

    my_tools = tools_to_use

    """
    model = TransformersModel(
        model_id=model_id,
        device_map="auto",
        torch_dtype="auto",
        #attn_implementation="eager",
    )
    """
    agent = CodeAgent(
        model=model, 
        #add_base_tools=True, 
        tools=my_tools,
        verbosity_level=LogLevel.INFO , #LogLevel.DEBUG, 
        max_steps=1
    )
    agent.run(task_prompt)

    agent_memory = agent.memory.get_full_steps()

    torch.cuda.empty_cache()

    return agent_memory


def prepare_for_inseq(memory_steps, tokenizer):
    """
    Extracts and formats the system + user prompts (formatted as the model sees them during inference)
    as well as the generated output from step 1 of smolagents memory.
    Preparation for inseq attribution.

    Args:
        memory_steps: list object as created by agent.memory.get_full_steps()
        tokenizer: A Transformers AutoTokenizer object
    
    Returns:
        input_text and output_text strings to be used by inseq
    """

    step1 = memory_steps[1]  # first step with actual model I/O
    messages: list[ChatMessage] = step1["model_input_messages"]
    assistant_msg: dict = step1["model_output_message"]

    # Flatten the content lists into strings
    def extract_text(msg: ChatMessage) -> str:
        if isinstance(msg.content, list):
            return "".join(
                part["text"] for part in msg.content if part["type"] == "text"
            )
        elif isinstance(msg.content, str):
            return msg.content
        return ""

    system_text = extract_text(next(m for m in messages if m.role.name == "SYSTEM"))
    user_text   = extract_text(next(m for m in messages if m.role.name == "USER"))

    # Format input exactly like Qwen sees it
    input_text = tokenizer.apply_chat_template(
        [
            {"role": "system", "content": system_text},
            {"role": "user", "content": user_text},
        ],
        tokenize=False,
        add_generation_prompt=True,  # ensures <|assistant|> token is included
    )

    # Assistant output (already a string in smolagents memory)
    output_text = assistant_msg["content"]

    return input_text, output_text


def run_inseq_analysis(prompt: str, generated_text: str, inseq_model: inseq.AttributionModel) -> object:
    """
    Runs the Inseq attribution analysis on an agent run.
    
    Args:
        prompt: The original input prompt (system prompt + task prompt).
        generated_text: The text generated by the agent.
        
    Returns:
        The complex attribution output object from Inseq.
    """
    print("  -> Running Inseq analysis...")

    """
    inseq_model = inseq.load_model(
        model_id,
        "saliency",
        #"attention",
        #attn_implementation="eager"
    )#"saliency")#"integrated_gradients") #die choice hier auch als argsparse machen, was dieser function als variable Ã¼berreicht wird
    """

    attribution_result = inseq_model.attribute(
        prompt,
        generated_texts=prompt + generated_text
        #skip_special_tokens=True ## was will ich hier?
    )
    
    return attribution_result


# ==============================================================================
# == 2. Execution Loop
# ==============================================================================

if __name__ == "__main__":
    # --- Load models ---
    # On a computer with enough memory, it could be better to load the models here (once)
    # and pass them to the functions. For both smolagents and inseq models.
    # On my machine (low memory, never calculating the whole dataset) its probably best this way.

    # --- Setup Argument Parser ---
    parser = argparse.ArgumentParser(description="Run agent and Inseq analysis for a specific category of tasks.")
    parser.add_argument("--category", type=str, required=True, help="The category name from the dataset (e.g., 'entertainment and media').")
    parser.add_argument("--model_name", type=str, required=True)
    args = parser.parse_args()

    # --- Select Model ID ---
    #model_id_short = "Qwen3-4B" #"Qwen3-1.7B" # "Qwen3-4B" # "gemma-3-4b-it"  <----------------- select model size here
    model_id_short = args.model_name
    model_id = "Qwen/" + model_id_short #"google/
    # --- Setup Tokenizer ---
    tokenizer = AutoTokenizer.from_pretrained(model_id)

    selected_category = args.category
    
    # --- Setup Paths and Directories ---
    TASKS_FILE = os.path.join("data", "dataset_combined.json")
    TOOL_LIST_FILE = os.path.join("data", "tool_list_combined.json")
    OUTPUT_DIR = os.path.join("outputs", selected_category.replace(" ", "_")) #selected_category.replace kann glaub weg

    os.makedirs(OUTPUT_DIR, exist_ok=True)
    
    print(f"Starting process for category: '{selected_category}'")
    
    # --- Load and Select Tools Dynamically ---
    try:
        with open(TOOL_LIST_FILE, 'r') as f:
            all_tools_by_category = json.load(f)
    except FileNotFoundError:
        print(f"ERROR: Tool list file not found at {TOOL_LIST_FILE}")
        exit()
        
    # Get the tools for the selected category
    tools_for_category = all_tools_by_category.get(selected_category)
    if not tools_for_category:
        print(f"ERROR: Category '{selected_category}' not found in {TOOL_LIST_FILE}")
        exit()
    print(f"Found {len(tools_for_category)} tools for category '{selected_category}'.")
    
    # Get a flat list of all other tools
    other_tools = [
        tool_name
        for category, tool_list in all_tools_by_category.items()
        for tool_name in tool_list
        if category != selected_category
    ]
    
    # Get a random sample of 20 other tools
    num_random_tools = 10 # <----------------------------- number of random added tools (20)
    if len(other_tools) > num_random_tools:
        random_sample = random.sample(other_tools, num_random_tools)
        print(f"Adding a random sample of {len(random_sample)} tools from other categories.")
    else:
        random_sample = other_tools  # Take all if less than 20 are available
        print(f"Adding all {len(random_sample)} available tools from other categories (fewer than {num_random_tools}).")

    # Combine the lists of tool names and remove duplicates
    final_tool_names = list(set(tools_for_category + random_sample))

    # Map tool names (strings) to actual function objects
    tool_functions = []
    for tool_name in final_tool_names:
        # Assumes 'from all_smol_agent_tools import *' has put the functions in the global scope
        tool_func = globals().get(tool_name)
        if tool_func and callable(tool_func):
             tool_functions.append(tool_func)
        else:
            print(f"  -> WARNING: Tool function '{tool_name}' not found and will be skipped.")
    
    # --- Load Tasks ---
    try:
        with open(TASKS_FILE, 'r') as f:
            all_task_records = json.load(f)
    except FileNotFoundError:
        print(f"ERROR: Tasks file not found at {TASKS_FILE}")
        exit()
        
    tasks_to_run = [
        record 
        for record in all_task_records 
        if record.get("category") == selected_category
    ]
        
    if not tasks_to_run:
        print(f"ERROR: No tasks found for category '{selected_category}' in {TASKS_FILE}")
        exit()
        
    print(f"Found {len(tasks_to_run)} tasks for this category.")

    print(f"\n--- Loading model {model_id} into memory... ---")
    # Load the model for the agent
    agent_model = TransformersModel(
        model_id=model_id,
        device_map="auto",
        torch_dtype="auto",
    )

    inseq_model = inseq.load_model(
        model=agent_model.model,
        attribution_method="saliency",
        tokenizer=agent_model.tokenizer,
        #"attention",
    )
    
    # --- Main Processing Loop ---
    for i, task_record in enumerate(tasks_to_run):
        
        # 1. Extract the id and instruction from the record
        task_id = task_record.get("my_id", f"unknownID_{i}")#"id" # Fallback in case id is missing
        task_prompt = task_record.get("instruction", "")

        if not task_prompt:
            print(f"  -> WARNING: Skipping record with ID {task_id} due to missing 'instruction'.")
            continue

        print(f"\n--- Processing Task {i+1}/{len(tasks_to_run)} (ID: {task_id}) ---")
        
        timestamp = datetime.now().strftime("%d-%m_%H-%M-%S")
        unique_id = f"{selected_category.replace(' ', '_')}_ID{task_id}_{model_id_short}_{timestamp}"
        
        # 2. Run the agent
        agent_output_log = run_my_agent(task_prompt, tool_functions, agent_model)
        
        # 3. Save the agent's full log output
        agent_output_filename = os.path.join(OUTPUT_DIR, f"{unique_id}_agent_out.dill")
        with open(agent_output_filename, "wb") as f:
            dill.dump(agent_output_log, f)
        print(f"  -> Agent log saved to: {agent_output_filename}")

        # 4. Preprocess the models memory to be used as strings by inseq
        agent_input_string, agent_output_string = prepare_for_inseq(agent_output_log, tokenizer)
        
        # 5. Run the Inseq analysis
        inseq_output_object = run_inseq_analysis(agent_input_string, agent_output_string, inseq_model)
        
        # 5. Save the complex Inseq object using dill
        inseq_output_filename = os.path.join(OUTPUT_DIR, f"{unique_id}_inseq_out.dill")
        with open(inseq_output_filename, "wb") as f:
            dill.dump(inseq_output_object, f)
        print(f"  -> Inseq object saved to: {inseq_output_filename}")

    print("\n All tasks for the category completed successfully!")
