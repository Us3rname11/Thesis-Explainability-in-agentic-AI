{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Analyzing the Agent Task Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------- Dataset Analytics Results ---------\n",
      "\n",
      "[1] Summary Matrix:\n",
      "                           Records  Unique Tasks\n",
      "business_and_productivity       50            20\n",
      "data                            50            20\n",
      "entertainment_and_media         50            20\n",
      "finance                         50            20\n",
      "travel_and_transportation       50            20\n",
      "--- TOTAL ---                  250           100\n",
      "\n",
      "[2] Records per unique task (grouped by category):\n",
      "    - business_and_productivity:\n",
      "        - apply_for_job: 3 records\n",
      "        - auto_housework_by_robot: 3 records\n",
      "        - borrow_book_online: 2 records\n",
      "        - consult_lawyer_online: 2 records\n",
      "        - create_team_poll: 3 records\n",
      "        - enroll_in_course: 3 records\n",
      "        - generate_meeting_summary: 3 records\n",
      "        - initiate_code_repository: 2 records\n",
      "        - log_work_hours: 2 records\n",
      "        - manage_crm_contact: 3 records\n",
      "        - organize_meeting_online: 2 records\n",
      "        - print_document: 3 records\n",
      "        - query_internal_knowledge_base: 2 records\n",
      "        - schedule_follow_up_communication: 2 records\n",
      "        - sell_item_online: 3 records\n",
      "        - send_email: 2 records\n",
      "        - set_alarm: 2 records\n",
      "        - software_management: 2 records\n",
      "        - track_project_progress: 3 records\n",
      "        - transcribe_meeting_audio: 3 records\n",
      "    - data:\n",
      "        - clean_dataset: 3 records\n",
      "        - compare_two_documents: 2 records\n",
      "        - compress_file: 2 records\n",
      "        - convert_file_format: 3 records\n",
      "        - create_qr_code: 2 records\n",
      "        - create_spreadsheet_from_data: 3 records\n",
      "        - extract_data_from_website: 2 records\n",
      "        - extract_tabular_data_from_pdf: 3 records\n",
      "        - extract_text_from_image: 3 records\n",
      "        - generate_chart_from_data: 3 records\n",
      "        - get_data_from_public_api: 2 records\n",
      "        - get_news_for_topic: 3 records\n",
      "        - merge_documents: 2 records\n",
      "        - perform_sentiment_analysis_on_text: 2 records\n",
      "        - query_dataset_with_natural_language: 3 records\n",
      "        - recording_audio: 2 records\n",
      "        - redact_sensitive_information: 2 records\n",
      "        - search_by_engine: 2 records\n",
      "        - summarize_article_from_url: 3 records\n",
      "        - take_note: 3 records\n",
      "    - entertainment_and_media:\n",
      "        - attend_meeting_online: 2 records\n",
      "        - book_movie_tickets: 3 records\n",
      "        - find_local_events_and_concerts: 2 records\n",
      "        - find_podcast_on_topic: 3 records\n",
      "        - get_latest_sports_scores: 3 records\n",
      "        - get_lyrics_for_song: 2 records\n",
      "        - get_random_trivia_question: 2 records\n",
      "        - get_tv_show_recommendations: 3 records\n",
      "        - identify_song_playing: 2 records\n",
      "        - make_video_call: 3 records\n",
      "        - make_voice_call: 2 records\n",
      "        - online_shopping: 3 records\n",
      "        - order_food_delivery: 3 records\n",
      "        - play_movie_by_title: 3 records\n",
      "        - play_music_by_title: 3 records\n",
      "        - read_aloud_news_headlines: 2 records\n",
      "        - see_doctor_online: 2 records\n",
      "        - send_sms: 2 records\n",
      "        - share_by_social_network: 2 records\n",
      "        - start_multiplayer_game: 3 records\n",
      "    - finance:\n",
      "        - analyze_monthly_spending: 3 records\n",
      "        - automate_loan_payment: 2 records\n",
      "        - buy_insurance: 2 records\n",
      "        - check_credit_score: 2 records\n",
      "        - convert_currency: 4 records\n",
      "        - create_savings_goal: 2 records\n",
      "        - daily_bill_payment: 2 records\n",
      "        - do_tax_return: 2 records\n",
      "        - find_nearby_atm: 2 records\n",
      "        - get_crypto_price: 4 records\n",
      "        - get_stock_quote: 3 records\n",
      "        - identify_tax_deductible_expenses: 2 records\n",
      "        - online_banking: 2 records\n",
      "        - pay_for_credit_card: 2 records\n",
      "        - search_transaction_history: 3 records\n",
      "        - send_money_to_contact: 3 records\n",
      "        - set_budget_for_category: 3 records\n",
      "        - split_bill_with_friends: 2 records\n",
      "        - stock_operation: 2 records\n",
      "        - track_investment_portfolio: 3 records\n",
      "    - travel_and_transportation:\n",
      "        - add_item_to_trip_itinerary: 3 records\n",
      "        - apply_for_passport: 2 records\n",
      "        - auto_driving_to_destination: 3 records\n",
      "        - book_car: 2 records\n",
      "        - book_flight: 2 records\n",
      "        - book_hotel: 2 records\n",
      "        - book_restaurant: 3 records\n",
      "        - book_train_ticket: 2 records\n",
      "        - check_flight_status: 3 records\n",
      "        - compare_rental_car_prices: 2 records\n",
      "        - deliver_package: 3 records\n",
      "        - find_airport_lounge_access: 2 records\n",
      "        - find_nearby_electric_vehicle_charger: 3 records\n",
      "        - find_travel_visa_requirements: 3 records\n",
      "        - get_real_time_traffic_update: 3 records\n",
      "        - get_weather: 3 records\n",
      "        - order_taxi: 2 records\n",
      "        - plan_public_transport_route: 3 records\n",
      "        - track_luggage: 2 records\n",
      "        - translate_sign_or_menu_with_camera: 2 records\n",
      "\n",
      "-------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "from collections import defaultdict\n",
    "import pandas as pd\n",
    "\n",
    "def analyze_data(filepath):\n",
    "    \"\"\"\n",
    "    Analyzes a JSON dataset to count tasks and categories.\n",
    "\n",
    "    Args:\n",
    "        filepath (str): The path to the JSON dataset file.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        with open(filepath, 'r', encoding='utf-8') as f:\n",
    "            data = json.load(f)\n",
    "    except FileNotFoundError:\n",
    "        print(f\"Error: The file '{filepath}' was not found.\")\n",
    "        return\n",
    "    except json.JSONDecodeError:\n",
    "        print(f\"Error: The file '{filepath}' is not a valid JSON file.\")\n",
    "        return\n",
    "\n",
    "    # --- Data structures for analytics ---\n",
    "    unique_tasks = set()\n",
    "    category_to_tasks = defaultdict(set)\n",
    "    category_record_counts = defaultdict(int)\n",
    "    category_task_counts = defaultdict(lambda: defaultdict(int))\n",
    "\n",
    "    # --- Process each record in the dataset ---\n",
    "    for record in data:\n",
    "        category = record.get('category', 'Uncategorized')\n",
    "        category_record_counts[category] += 1\n",
    "        nodes = record.get('sampled_nodes', [])\n",
    "\n",
    "        if not isinstance(nodes, list):\n",
    "            continue\n",
    "\n",
    "        for node in nodes:\n",
    "            task = node.get('task')\n",
    "            if task:\n",
    "                unique_tasks.add(task)\n",
    "                category_to_tasks[category].add(task)\n",
    "                category_task_counts[category][task] += 1\n",
    "\n",
    "    # --- Display the results ---\n",
    "    print(\"--------- Dataset Analytics Results ---------\")\n",
    "\n",
    "    # --- NEW: Create and display the summary matrix using pandas ---\n",
    "    '''\n",
    "    print(f\"\\n[*] Total records analyzed: {len(data)}\")\n",
    "    print(f\"[*] Total unique tasks found: {len(unique_tasks)}\")\n",
    "    '''\n",
    "    \n",
    "    # Prepare data for the DataFrame\n",
    "    table_data = []\n",
    "    for category in sorted(category_record_counts.keys()):\n",
    "        table_data.append({\n",
    "            \"Category\": category,\n",
    "            \"Records\": category_record_counts[category],\n",
    "            \"Unique Tasks\": len(category_to_tasks.get(category, set()))\n",
    "        })\n",
    "\n",
    "    if table_data:\n",
    "        # Create DataFrame and set the Category as the index\n",
    "        df = pd.DataFrame(table_data)\n",
    "        df = df.set_index(\"Category\")\n",
    "\n",
    "        # Add a \"TOTAL\" row at the end\n",
    "        total_row = pd.DataFrame({\n",
    "            'Records': [len(data)],\n",
    "            'Unique Tasks': [len(unique_tasks)]\n",
    "        }, index=['--- TOTAL ---'])\n",
    "\n",
    "        # Concatenate the main dataframe with the total row\n",
    "        df = pd.concat([df, total_row])\n",
    "\n",
    "        print(\"\\n[1] Summary Matrix:\")\n",
    "        # Use to_string() to ensure the full table is printed\n",
    "        print(df.to_string())\n",
    "    else:\n",
    "        print(\"\\n[1] Summary Matrix: No data to display.\")\n",
    "\n",
    "\n",
    "    # --- Display detailed breakdown, grouped by category ---\n",
    "    print(\"\\n[2] Records per unique task (grouped by category):\")\n",
    "    # Iterate through each category, sorted alphabetically\n",
    "    for category, tasks in sorted(category_task_counts.items()):\n",
    "        print(f\"    - {category}:\")\n",
    "        # For each category, iterate through its tasks, sorted alphabetically\n",
    "        for task, count in sorted(tasks.items()):\n",
    "            print(f\"        - {task}: {count} records\")\n",
    "\n",
    "    print(\"\\n-------------------------------------------\")\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    DATASET_FILE = \"/Users/val/MA/Code/main_folder/data/dataset_combined.json\"\n",
    "    analyze_data(DATASET_FILE)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Analyzing the model output files for completeness"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scanning subdirectories in: 'outputs23-09'...\n",
      "\n",
      "  Checking 'business_and_productivity'...\n",
      "  Checking 'travel_and_transportation'...\n",
      "  Checking 'entertainment_and_media'...\n",
      "  Checking 'finance'...\n",
      "  Checking 'data'...\n",
      "\n",
      "------------------------------\n",
      "Analysis complete. Found a total of 250 unique IDs.\n",
      "------------------------------\n",
      "❌ Found 30 ID(s) that are missing corresponding files:\n",
      "\n",
      "finance:\n",
      "- ID1072 file/s missing: Model Qwen3-1.7B, Filetype/s inseq_out.dill\n",
      "- ID1073 file/s missing: Model Qwen3-1.7B, Filetype/s agent_out.dill + inseq_out.dill\n",
      "- ID1074 file/s missing: Model Qwen3-1.7B, Filetype/s agent_out.dill + inseq_out.dill\n",
      "- ID1075 file/s missing: Model Qwen3-1.7B, Filetype/s agent_out.dill + inseq_out.dill\n",
      "- ID1076 file/s missing: Model Qwen3-1.7B, Filetype/s agent_out.dill + inseq_out.dill\n",
      "- ID1077 file/s missing: Model Qwen3-1.7B, Filetype/s agent_out.dill + inseq_out.dill\n",
      "- ID1078 file/s missing: Model Qwen3-1.7B, Filetype/s agent_out.dill + inseq_out.dill\n",
      "- ID1079 file/s missing: Model Qwen3-1.7B, Filetype/s agent_out.dill + inseq_out.dill\n",
      "- ID1080 file/s missing: Model Qwen3-1.7B, Filetype/s agent_out.dill + inseq_out.dill\n",
      "- ID1081 file/s missing: Model Qwen3-1.7B, Filetype/s agent_out.dill + inseq_out.dill\n",
      "- ID1082 file/s missing: Model Qwen3-1.7B, Filetype/s agent_out.dill + inseq_out.dill\n",
      "- ID1083 file/s missing: Model Qwen3-1.7B, Filetype/s agent_out.dill + inseq_out.dill\n",
      "- ID1084 file/s missing: Model Qwen3-1.7B, Filetype/s agent_out.dill + inseq_out.dill\n",
      "- ID1085 file/s missing: Model Qwen3-1.7B, Filetype/s agent_out.dill + inseq_out.dill\n",
      "- ID1086 file/s missing: Model Qwen3-1.7B, Filetype/s agent_out.dill + inseq_out.dill\n",
      "- ID1087 file/s missing: Model Qwen3-1.7B, Filetype/s agent_out.dill + inseq_out.dill\n",
      "- ID1088 file/s missing: Model Qwen3-1.7B, Filetype/s agent_out.dill + inseq_out.dill\n",
      "- ID1089 file/s missing: Model Qwen3-1.7B, Filetype/s agent_out.dill + inseq_out.dill\n",
      "- ID2012 file/s missing: Model Qwen3-1.7B, Filetype/s agent_out.dill + inseq_out.dill\n",
      "- ID2013 file/s missing: Model Qwen3-1.7B, Filetype/s agent_out.dill + inseq_out.dill\n",
      "- ID2019 file/s missing: Model Qwen3-1.7B, Filetype/s agent_out.dill + inseq_out.dill\n",
      "- ID2020 file/s missing: Model Qwen3-1.7B, Filetype/s agent_out.dill + inseq_out.dill\n",
      "- ID2029 file/s missing: Model Qwen3-1.7B, Filetype/s agent_out.dill + inseq_out.dill\n",
      "- ID2030 file/s missing: Model Qwen3-1.7B, Filetype/s agent_out.dill + inseq_out.dill\n",
      "- ID2046 file/s missing: Model Qwen3-1.7B, Filetype/s agent_out.dill + inseq_out.dill\n",
      "- ID2047 file/s missing: Model Qwen3-1.7B, Filetype/s agent_out.dill + inseq_out.dill\n",
      "- ID2061 file/s missing: Model Qwen3-1.7B, Filetype/s agent_out.dill + inseq_out.dill\n",
      "- ID2062 file/s missing: Model Qwen3-1.7B, Filetype/s agent_out.dill + inseq_out.dill\n",
      "- ID2090 file/s missing: Model Qwen3-1.7B, Filetype/s agent_out.dill + inseq_out.dill\n",
      "- ID2091 file/s missing: Model Qwen3-1.7B, Filetype/s agent_out.dill + inseq_out.dill\n",
      "------------------------------\n",
      "🔍 Found 3 ID(s) with duplicate files:\n",
      "\n",
      "data:\n",
      "- ID1138 has duplicates: Model Qwen3-1.7B, Filetype agent_out.dill (found 2 times)\n",
      "\n",
      "entertainment_and_media:\n",
      "- ID2052 has duplicates: Model Qwen3-0.6B, Filetype agent_out.dill (found 2 times)\n",
      "\n",
      "finance:\n",
      "- ID1058 has duplicates: Model Qwen3-1.7B, Filetype agent_out.dill (found 2 times)\n",
      "\n",
      "------------------------------\n",
      "Returned list of IDs with MISSING files:\n",
      "['1072', '1073', '1074', '1075', '1076', '1077', '1078', '1079', '1080', '1081', '1082', '1083', '1084', '1085', '1086', '1087', '1088', '1089', '2012', '2013', '2019', '2020', '2029', '2030', '2046', '2047', '2061', '2062', '2090', '2091']\n",
      "\n",
      "------------------------------\n",
      "Returned list of IDs with DUPLICATE files:\n",
      "['1058', '1138', '2052']\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from collections import defaultdict, Counter\n",
    "import itertools\n",
    "\n",
    "def analyze_files_in_subdirectories(parent_directory_path):\n",
    "    \"\"\"\n",
    "    Analyzes files in subdirectories to ensure each unique ID has the correct\n",
    "    number of files and to identify duplicates.\n",
    "\n",
    "    Args:\n",
    "        parent_directory_path (str): The path to the parent directory to scan.\n",
    "\n",
    "    Returns:\n",
    "        tuple[list, list]: A tuple containing two lists:\n",
    "                           1. A sorted list of string ID numbers with missing files.\n",
    "                           2. A sorted list of string ID numbers with duplicate files.\n",
    "    \"\"\"\n",
    "    if not os.path.isdir(parent_directory_path):\n",
    "        print(f\"Error: Parent directory not found at '{parent_directory_path}'\")\n",
    "        return [], []\n",
    "\n",
    "    # Use a list for combinations to allow for duplicate detection.\n",
    "    id_data = defaultdict(lambda: {\"category\": None, \"combinations\": []})\n",
    "\n",
    "    expected_models = {'Qwen3-0.6B', 'Qwen3-1.7B'}\n",
    "    expected_endings = {'agent_out.dill', 'inseq_out.dill'}\n",
    "    expected_combinations = set(itertools.product(expected_models, expected_endings))\n",
    "\n",
    "    print(f\"Scanning subdirectories in: '{parent_directory_path}'...\\n\")\n",
    "    for category_dir in os.listdir(parent_directory_path):\n",
    "        category_path = os.path.join(parent_directory_path, category_dir)\n",
    "        if os.path.isdir(category_path):\n",
    "            print(f\"  Checking '{category_dir}'...\")\n",
    "            for filename in os.listdir(category_path):\n",
    "                filepath = os.path.join(category_path, filename)\n",
    "                if os.path.isfile(filepath):\n",
    "                    parts = filename.split('_')\n",
    "                    file_id, model_name = None, None\n",
    "                    for part in parts:\n",
    "                        if part.startswith(\"ID\"):\n",
    "                            file_id = part\n",
    "                        elif part.startswith(\"Qwen3\"):\n",
    "                            model_name = part\n",
    "                    \n",
    "                    if not file_id or not model_name:\n",
    "                        continue\n",
    "\n",
    "                    file_ending = None\n",
    "                    if filename.endswith('agent_out.dill'):\n",
    "                        file_ending = 'agent_out.dill'\n",
    "                    elif filename.endswith('inseq_out.dill'):\n",
    "                        file_ending = 'inseq_out.dill'\n",
    "                    \n",
    "                    if file_ending:\n",
    "                        id_data[file_id][\"category\"] = category_dir\n",
    "                        id_data[file_id][\"combinations\"].append((model_name, file_ending))\n",
    "\n",
    "    if not id_data:\n",
    "        print(\"\\nNo files matching the required ID and Model pattern were found.\")\n",
    "        return [], []\n",
    "        \n",
    "    total_unique_ids = len(id_data)\n",
    "    print(\"\\n\" + \"-\" * 30)\n",
    "    print(f\"Analysis complete. Found a total of {total_unique_ids} unique IDs.\")\n",
    "    print(\"-\" * 30)\n",
    "\n",
    "    # --- Analysis and Segregation of IDs ---\n",
    "    incomplete_ids = {}\n",
    "    duplicate_ids = {}\n",
    "\n",
    "    for file_id, details in id_data.items():\n",
    "        combinations_list = details[\"combinations\"]\n",
    "        unique_combinations = set(combinations_list)\n",
    "\n",
    "        # Check for missing files\n",
    "        if unique_combinations != expected_combinations:\n",
    "            incomplete_ids[file_id] = details\n",
    "\n",
    "        # Check for duplicate files\n",
    "        if len(combinations_list) > len(unique_combinations):\n",
    "            duplicate_ids[file_id] = details\n",
    "\n",
    "    # --- Reporting Section ---\n",
    "    all_ok = True\n",
    "    # Report 1: Incomplete IDs\n",
    "    if not incomplete_ids:\n",
    "        print(\"✅ Success! No IDs are missing files.\")\n",
    "    else:\n",
    "        all_ok = False\n",
    "        incomplete_by_category = defaultdict(list)\n",
    "        for file_id, details in incomplete_ids.items():\n",
    "            incomplete_by_category[details[\"category\"]].append((file_id, details))\n",
    "\n",
    "        print(f\"❌ Found {len(incomplete_ids)} ID(s) that are missing corresponding files:\")\n",
    "        for category, id_list in sorted(incomplete_by_category.items()):\n",
    "            print(f\"\\n{category}:\")\n",
    "            for file_id, details in sorted(id_list):\n",
    "                missing_combinations = expected_combinations - set(details[\"combinations\"])\n",
    "                missing_by_model = defaultdict(list)\n",
    "                for model, ending in missing_combinations:\n",
    "                    missing_by_model[model].append(ending)\n",
    "                report_parts = []\n",
    "                for model, endings in sorted(missing_by_model.items()):\n",
    "                    report_parts.append(f\"Model {model}, Filetype/s {' + '.join(sorted(endings))}\")\n",
    "                print(f\"- {file_id} file/s missing: {'; '.join(report_parts)}\")\n",
    "    \n",
    "    print(\"-\" * 30)\n",
    "    # Report 2: Duplicate IDs\n",
    "    if not duplicate_ids:\n",
    "        print(\"✅ Success! No IDs have duplicate files.\")\n",
    "    else:\n",
    "        all_ok = False\n",
    "        duplicates_by_category = defaultdict(list)\n",
    "        for file_id, details in duplicate_ids.items():\n",
    "            duplicates_by_category[details[\"category\"]].append((file_id, details))\n",
    "\n",
    "        print(f\"🔍 Found {len(duplicate_ids)} ID(s) with duplicate files:\")\n",
    "        for category, id_list in sorted(duplicates_by_category.items()):\n",
    "            print(f\"\\n{category}:\")\n",
    "            for file_id, details in sorted(id_list):\n",
    "                counts = Counter(details[\"combinations\"])\n",
    "                dupes = {item: count for item, count in counts.items() if count > 1}\n",
    "                report_parts = []\n",
    "                for (model, ending), count in dupes.items():\n",
    "                    report_parts.append(f\"Model {model}, Filetype {ending} (found {count} times)\")\n",
    "                print(f\"- {file_id} has duplicates: {'; '.join(report_parts)}\")\n",
    "\n",
    "    if all_ok:\n",
    "        print(\"\\n✅✅ Overall Success! All IDs have exactly 4 unique files.\")\n",
    "\n",
    "    # --- Return Section ---\n",
    "    incomplete_keys = [str(int(key.replace('ID', ''))) for key in incomplete_ids.keys()]\n",
    "    duplicate_keys = [str(int(key.replace('ID', ''))) for key in duplicate_ids.keys()]\n",
    "    \n",
    "    return sorted(incomplete_keys, key=int), sorted(duplicate_keys, key=int)\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    parent_directory = \"outputs23-09\"\n",
    "    \n",
    "    missing_ids_list, duplicate_ids_list = analyze_files_in_subdirectories(parent_directory)\n",
    "\n",
    "    if missing_ids_list:\n",
    "        print(\"\\n\" + \"-\" * 30)\n",
    "        print(\"Returned list of IDs with MISSING files:\")\n",
    "        print(missing_ids_list)\n",
    "\n",
    "    if duplicate_ids_list:\n",
    "        print(\"\\n\" + \"-\" * 30)\n",
    "        print(\"Returned list of IDs with DUPLICATE files:\")\n",
    "        print(duplicate_ids_list)\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py312",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "6bfb36e1809ebaabb37e5aa29ecba9279008a69c1743045940bd5d510f792dfd"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
